# Wedge Co-Op Data Engineering Project

## Overview

This project is a deep dive into the world of data engineering. As a soon-to-be graduate of the MSBA program, I will spend a significant portion of my time shaping up data sets for analyses—approximately 80% of the job. A thorough understanding of data pipelines and how raw data is transformed into consumable formats is critical for success in a data engineering career.

In this project, I was tasked with defining the data sets needed for my work and transforming raw transactional data into actionable insights. This project hones those essential skills for any data engineer.

## About the Wedge Co-Op

The **Wedge Co-Op** is the largest co-operative grocery store in the United States and is located in Minneapolis, MN. Through a partnership with the co-op, I had access to data from their point-of-sale (POS) system dating back to January 1, 2010, and running through January 2017. This system logs every row of every receipt, providing a rich transaction-level view of the data.

Although I did not have access to other operational data like ordering, inventory, or spoilage, the transaction-level data is one of the richest available. This is because the Wedge is a member-owned co-operative. While the public is welcome to shop at the co-op, members enjoy additional benefits such as:
- Membership for $80 (payable in installments)
- $3.75 off per month
- Member discounts and annual refunds based on the co-op's profitability

## Key Dataset Insights

- About 75% of the transactions are generated by **owners** (members). For these owners, I had a comprehensive record of their shopping history, enabling me to analyze consumption changes over time and explore dimensions such as products purchased and departments shopped.

## Challenges with the Raw Data

Although the data is incredibly valuable, working with raw POS data presents its own challenges. The transaction records contain many non-item entries such as:
- Payment information
- Tax
- Discounts
- Members rounding up for charity
- Change given

Therefore, part of this project involved cleaning and transforming this data to extract meaningful insights.

## Data Format

The transaction files contain many columns. Many of these I didn’t need for my purposes, but there is a comprehensive list in the first appendix.

The data are stored in delimited text files. Unfortunately, some files are delimited with commas, and some are delimited with semicolons. Blank fields are typically given the value of “NULL,” but the Wedge occasionally uses “\N” or “\\N”. The files cover one to three months. Early on, the Wedge was splitting out inactive owner transactions, but in 2015, they switched to keeping those in the same file.

I saw both regular owner numbers and non-owners (card_no==3) in these files.

## Tasks

### Task 1: Building a Transaction Database in Google BigQuery

Google BigQuery is a distributed data warehouse built on a serverless architecture. In this task, I uploaded all Wedge transaction records to Google BigQuery. I ensured that the column data types were correctly specified and that I properly handled the null values.

The requirements for this task change depending on the grade I was going for.

**Note**: This assignment can be done manually or programmatically. I chose to do it programmatically for practice, but that is not required for full credit.

### Task 2: A Sample of Owners

The files are not easy to use in their current chronological arrangement, though having them in a large system like GBQ helped solve a lot of the problems. Nevertheless, it was convenient to have a local sample of owners for further work.

This task asked me to generate a file of owners where the file contains every record for each owner. There would be more than one owner in the file, and I excluded card_no==3 (the code for non-owners). I aimed for a sample around 250 MB. That was big enough to be rich, but small enough to process quickly.

#### Deliverable

A Python script that handled the following tasks:
- Connected to my GBQ instance.
- Built a list of owners.
- Took a sample of the owners.
- Extracted all records associated with those owners and wrote them to a local text file.

### Task 3: Building Summary Tables

It was useful to have summary files that allowed me to quickly answer questions such as:
- How have our sales-by-day changed over the last few months?
- What is our most popular item in each department?
- Which owners spend the most per month in each department?

The classic way to structure data to answer these questions is in a relational database. In this task, I built summary text files that held this data and populated a relational database with the information.

#### Input

I processed the owner records in GBQ to build the summary tables.

#### Output

For this task, I built a single SQLite database (in a `.db` file) containing three tables:

1. **Sales by date by hour**: By calendar date (YYYY-MM-DD) and hour of the day, I determined the total spend in the store, the number of transactions, and a count of the number of items.
2. **Sales by owner by year by month**: A table with the following columns: card_no, year, month, sales, transactions, and items.
3. **Sales by product description by year by month**: A table with the following columns: upc, description, department number, department name, year, month, sales, transactions, and items.

#### Deliverable

I submitted:
- The Python code that created the summary tables.
- The Python code that built the SQLite database.

**Note**: I generated these tables via queries in Google BigQuery, exported the text files, and stored them locally on my machine. I then wrote a Python script that created the database, created the tables, and filled those tables. The entire task was done programmatically in Python.

